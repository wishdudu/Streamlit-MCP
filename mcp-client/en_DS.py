# ------------------ ç¯å¢ƒä¸ä¾èµ–é…ç½® ------------------
import sys
import os
import json
import asyncio
import tempfile
from dotenv import load_dotenv

# Windows å¹³å°å…¼å®¹
if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

load_dotenv()

# ------------------ ç¬¬ä¸‰æ–¹ä¾èµ–å¯¼å…¥ ------------------
import streamlit as st
from contextlib import AsyncExitStack
from openai import OpenAI

from langchain.memory import ConversationBufferMemory
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain_community.document_loaders import TextLoader, PyMuPDFLoader, UnstructuredWordDocumentLoader, UnstructuredMarkdownLoader
from langchain_community.embeddings import DashScopeEmbeddings
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.callbacks.streamlit import StreamlitCallbackHandler

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

# ------------------ è‡ªå®šä¹‰ragå·¥å…·åŒ…è£…ç»“æ„ ------------------
class CustomRetrieverTool:
    def __init__(self, retriever):
        self.name = "rag"
        self.description = "ç”¨äºä»ä¸Šä¼ æ–‡æ¡£ä¸­æ£€ç´¢ç­”æ¡ˆ"
        self.retriever = retriever

    def run(self, query: str):
        docs = self.retriever.get_relevant_documents(query)
        result = "\n\n".join([doc.page_content for doc in docs[:3]]) if docs else "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚"

        # æ¨¡æ‹Ÿ .content[0].text çš„ç»“æ„
        class MockResponse:
            def __init__(self, text):
                self.content = [type("Text", (), {"text": text})()]

        return MockResponse(result)

# ------------------ æ–‡æ¡£å‘é‡æ£€ç´¢å™¨æ„å»º ------------------
@st.cache_resource(ttl="1h")
def configure_retriever(files):
    if not files:
        return None

    docs = []
    temp_dir = tempfile.TemporaryDirectory(dir="D:\\")
    for file in files:
        file_path = os.path.join(temp_dir.name, file.name)
        with open(file_path, "wb") as f:
            f.write(file.getvalue())
        ext = os.path.splitext(file.name)[1].lower()
        try:
            if ext == ".txt":
                loader = TextLoader(file_path, encoding="utf-8")
            elif ext == ".pdf":
                loader = PyMuPDFLoader(file_path)
            elif ext == ".docx":
                loader = UnstructuredWordDocumentLoader(file_path)
            elif ext == ".md":
                loader = UnstructuredMarkdownLoader(file_path)
            else:
                st.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file.name}")
                continue
            docs.extend(loader.load())
        except Exception as e:
            st.error(f"åŠ è½½æ–‡ä»¶ {file.name} å¤±è´¥ï¼š{e}")

    if not docs:
        return None

    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = splitter.split_documents(docs)
    embeddings = DashScopeEmbeddings()
    vectordb = Chroma.from_documents(splits, embeddings, persist_directory="./chroma_db")
    return vectordb.as_retriever()

# ------------------ MCP å®¢æˆ·ç«¯å°è£… ------------------
class MCPClient:
    def __init__(self, memory=None):
        # åˆå§‹åŒ–ä¼šè¯å’Œå®¢æˆ·ç«¯å¯¹è±¡
        self.deepseek_api_key = os.getenv("DEEPSEEK_API_KEY")
        self.base_url = os.getenv("BASE_URL")
        self.model = os.getenv("MODEL")

        if not self.deepseek_api_key:
            raise ValueError("âŒ æœªæ‰¾åˆ° DeepSeek API Keyï¼Œè¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® DEEPSEEK_API_KEY")

        self.client = OpenAI(api_key=self.deepseek_api_key, base_url=self.base_url)
        self.exit_stack = AsyncExitStack()
        self.memory = memory
        self.sessions = {}  # å­˜å‚¨å¤šä¸ªæœåŠ¡ç«¯ä¼šè¯
        self.tools_map = {}  # å·¥å…·æ˜ å°„ï¼šå·¥å…·åç§° -> {"server_id": server_id, "tool_obj": tool}
        self.custom_tools = {}  # å­˜å‚¨è‡ªå®šä¹‰å·¥å…·ï¼Œå¦‚rag

    def add_custom_tool(self, name, tool):
        self.custom_tools[name] = tool

    async def connect_to_server(self, server_id):
        """è¿æ¥åˆ° MCP æœåŠ¡å™¨,ä» .env åŠ è½½æœåŠ¡å™¨é…ç½®"""
        command = os.getenv(f"{server_id.upper()}_SERVER_COMMAND")
        args_str = os.getenv(f"{server_id.upper()}_SERVER_ARGS")
        args = [arg.strip() for arg in args_str.split(",")]
        env_str = os.getenv(f"{server_id.upper()}_SERVER_ENV")
        if env_str:
            env = dict(item.strip().split("=", 1) for item in env_str.split(",") if "=" in item)
            server_params = StdioServerParameters(command=command, args=args, env=env)
        else:
            server_params = StdioServerParameters(command=command, args=args)
        # å¯åŠ¨ MCP æœåŠ¡å™¨å¹¶å»ºç«‹é€šä¿¡
        stdio, write = await self.exit_stack.enter_async_context(stdio_client(server_params))
        session = await self.exit_stack.enter_async_context(ClientSession(stdio, write))
        await session.initialize()
        self.sessions[server_id] = {"session": session, "stdio": stdio, "write": write}

        # æ›´æ–°å·¥å…·æ˜ å°„
        for tool in (await session.list_tools()).tools:
            self.tools_map[tool.name] = {
                "server_id": server_id,
                "tool_obj": tool
            }

    async def process_query(self, query):
        """ä½¿ç”¨ DeepSeek å’Œå¯ç”¨çš„å·¥å…·å¤„ç†æŸ¥è¯¢"""
        # æ ¹æ®æ˜¯å¦åŠ è½½äº†RAGå·¥å…·åŠ¨æ€ç”Ÿæˆ system prompt
        if "rag" in self.custom_tools:
            instruction = '''ä½ æ˜¯ä¸€ä¸ªç»“åˆäº†æ–‡æ¡£æ£€ç´¢ï¼ˆragï¼‰ä¸mcpå·¥å…·è°ƒç”¨çš„æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹è¡Œä¸ºå‡†åˆ™è¿›è¡Œå·¥ä½œï¼š
        1. çŸ¥è¯†åº“å¤„ç†ï¼š
        ç”¨æˆ·å·²ä¸Šä¼ çŸ¥è¯†åº“æ–‡æ¡£ï¼Œæ— è®ºç”¨æˆ·è¾“å…¥ä¸ºä»€ä¹ˆï¼Œè¯·ä½¿ç”¨ragå·¥å…·è·å–ç›¸å…³ä¿¡æ¯ï¼Œå¹¶åŸºäºæ£€ç´¢ç»“æœè¿›è¡Œå›ç­”ã€‚
        2. å·¥å…·è°ƒç”¨åŸåˆ™ï¼š
        æ— è®ºæ˜¯å¦ä½¿ç”¨RAGå·¥å…·ï¼Œä½ éƒ½å¯ä»¥è°ƒç”¨å¯ç”¨çš„MCPå·¥å…·æ¥è¾…åŠ©å®Œæˆä»»åŠ¡ã€‚
        3. å›ç­”è¦æ±‚ï¼š
        - ç»“åˆæ£€ç´¢ä¿¡æ¯å’Œå·¥å…·ç»“æœè¿›è¡Œæ¸…æ™°ã€æœ‰é€»è¾‘çš„å›ç­”ã€‚
        - å¦‚æ£€ç´¢ç»“æœæ— å…³ç´§è¦ï¼Œè¯·ä¾é MCPå·¥å…·æˆ–è‡ªèº«çŸ¥è¯†ä½œç­”ã€‚
        - è‹¥æ— æœ‰æ•ˆç­”æ¡ˆï¼Œä¹Ÿè¯·å¦ç‡è¯´æ˜å¹¶æå‡ºåˆç†å»ºè®®ã€‚
        '''
        else:
            instruction = '''ä½ æ˜¯ä¸€ä¸ªç»“åˆäº†å¤–éƒ¨mcpå·¥å…·è°ƒç”¨èƒ½åŠ›çš„æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹è¡Œä¸ºå‡†åˆ™è¿›è¡Œå·¥ä½œï¼š
        1. å·¥å…·è°ƒç”¨åŸåˆ™ï¼š
        ä½ å¯ä»¥è°ƒç”¨ä½ æ‰€èƒ½è®¿é—®çš„MCPå·¥å…·æ¥è¾…åŠ©å›ç­”é—®é¢˜ã€‚
        2. å›ç­”è¦æ±‚ï¼š
        - ä¸»åŠ¨åˆ¤æ–­å¹¶è°ƒç”¨å¯ç”¨å·¥å…·å®Œæˆä»»åŠ¡ã€‚
        - å›ç­”éœ€æ¸…æ™°ã€æœ‰é€»è¾‘ï¼Œå°½å¯èƒ½å‡†ç¡®ã€‚
        - è‹¥æ— æœ‰æ•ˆä¿¡æ¯ï¼Œä¹Ÿè¯·è¯´æ˜æƒ…å†µå¹¶æå‡ºåˆç†å»ºè®®ã€‚
        '''

        history = []
        if self.memory:
            for msg in self.memory.chat_memory.messages:
                if msg.type == "human":
                    history.append({"role": "user", "content": msg.content})
                elif msg.type == "ai":
                    history.append({"role": "assistant", "content": msg.content})

        messages = [{"role": "system", "content": instruction}] + history + [{"role": "user", "content": query}]

        # æ„å»ºç»Ÿä¸€çš„å·¥å…·åˆ—è¡¨,åˆ†åˆ«æ·»åŠ mcpå·¥å…·å’Œè‡ªå®šä¹‰å·¥å…·
        available_tools = []
        for tool_name, tool_info in self.tools_map.items():
            tool = tool_info["tool_obj"]
            available_tools.append({
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": tool.inputSchema
                }
            })

        for tool in self.custom_tools.values():
            available_tools.append({
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "query": {
                                "type": "string",
                                "description": "ç”¨æˆ·è¦æ£€ç´¢çš„é—®é¢˜"
                            }
                        },
                        "required": ["query"]
                    }
                }
            })

        # å¾ªç¯å¤„ç†å·¥å…·è°ƒç”¨
        while True:
            # åˆå§‹ DeepSeek API è°ƒç”¨
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                tools=available_tools,
                tool_choice="auto"
            )

            # å¤„ç†è¿”å›çš„å†…å®¹
            content = response.choices[0]
            if content.finish_reason == "tool_calls":
                # æ‰§è¡Œå·¥å…·è°ƒç”¨
                for tool_call in content.message.tool_calls:
                    tool_name = tool_call.function.name
                    tool_args = json.loads(tool_call.function.arguments)

                    # æ ¹æ®å·¥å…·åç§°è°ƒç”¨å¯¹åº”çš„æœåŠ¡ç«¯/è‡ªå®šä¹‰å·¥å…·
                    if tool_name in self.tools_map:
                        session = self.sessions[self.tools_map[tool_name]["server_id"]]["session"]
                        result = await session.call_tool(tool_name, tool_args)
                        print(f"\n[ <{tool_args}> è°ƒç”¨å·¥å…· <{tool_name}>]\n")
                        tool_obj = self.tools_map[tool_name]["tool_obj"]
                        print(f"[æ³¨å†Œå·¥å…·] {tool_obj.name} å‚æ•°ç»“æ„: {json.dumps(tool_obj.inputSchema, ensure_ascii=False, indent=2)}")
                        print(result)
                    elif tool_name in self.custom_tools:
                        result = self.custom_tools[tool_name].run(tool_args["query"])
                        print(f"\n[ <{tool_args}> è°ƒç”¨å·¥å…· <{tool_name}>]\n")
                        print(result)
                    # å°† tool çš„ç»“æœä½œä¸ºä¸€æ¡æ¶ˆæ¯å›ä¼ 
                    messages.append({"role": "assistant", "tool_calls": [tool_call]})
                    messages.append({
                        "role": "tool",
                        "content": result.content[0].text,
                        "tool_call_id": tool_call.id
                    })
            else:
                # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œå†™å…¥è®°å¿†å¹¶è¿”å›æœ€ç»ˆå›å¤
                if self.memory:
                    self.memory.chat_memory.add_user_message(query)
                    self.memory.chat_memory.add_ai_message(content.message.content)

                return content.message.content

# ------------------ MCP åˆå§‹åŒ– ------------------
@st.cache_resource
def init_mcp(_memory):
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    mcp = MCPClient(memory=memory)
    # æ­¤å¤„æ·»åŠ æ–°çš„mcp server
    loop.run_until_complete(mcp.connect_to_server("amap_maps"))
    print("Connected to server: amap_maps")
    loop.run_until_complete(mcp.connect_to_server("tavily"))
    print("Connected to server: tavily")
    loop.run_until_complete(mcp.connect_to_server("filesystem"))
    print("Connected to server: filesystem")
    loop.run_until_complete(mcp.connect_to_server("sequential_thinking"))
    print("Connected to server: sequential_thinking")
    loop.run_until_complete(mcp.connect_to_server("time"))
    print("Connected to server: time")


    return mcp, loop

# ------------------ Streamlit é¡µé¢é€»è¾‘ ------------------
st.set_page_config(page_title="DeepSeekå¢å¼ºç‰ˆ", layout="wide")
st.title("ğŸ“ DeepSeekå¢å¼ºç‰ˆ")

uploaded_files = st.sidebar.file_uploader(
    "ä¸Šä¼  RAGçŸ¥è¯†åº“æ–‡æ¡£ï¼ˆå¯é€‰ï¼‰",
    type=["txt", "pdf", "docx", "md"],
    accept_multiple_files=True,
)

retriever = configure_retriever(uploaded_files)

# åˆ›å»ºèŠå¤©æ¶ˆæ¯å†å²è®°å½•å’Œå¯¹è¯ç¼“å†²åŒºå†…å­˜
msgs = StreamlitChatMessageHistory()
memory = ConversationBufferMemory(chat_memory=msgs, return_messages=True, memory_key="chat_history", output_key="output")

# å¦‚æœsession_stateä¸­æ²¡æœ‰æ¶ˆæ¯è®°å½•æˆ–ç”¨æˆ·ç‚¹å‡»äº†æ¸…ç©ºèŠå¤©è®°å½•æŒ‰é’®ï¼Œåˆ™åˆå§‹åŒ–æ¶ˆæ¯è®°å½•å’Œè®°å¿†
if "messages" not in st.session_state or st.sidebar.button("æ¸…ç©ºèŠå¤©è®°å½•"):
    st.session_state["messages"] = [{"role": "assistant", "content": "ä½ å¥½ï¼Œæˆ‘æ˜¯æ–‡æ¡£é—®ç­”åŠ©æ‰‹ã€‚ æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„ï¼Ÿ"}]
    msgs.clear()  # æ¸…ç©ºå†å²å¯¹è¯

# å±•ç¤ºå†å²èŠå¤©è®°å½•
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

mcp_client, event_loop = init_mcp(memory)

if 'rag' in mcp_client.custom_tools:
    del mcp_client.custom_tools['rag']
# è‹¥ç”¨æˆ·ä¸Šä¼ çŸ¥è¯†åº“ï¼Œè°ƒç”¨çŸ¥è¯†åº“å·¥å…·
if retriever:
    rag_tool = CustomRetrieverTool(retriever)
    mcp_client.add_custom_tool(rag_tool.name, rag_tool)

# å¤„ç†ç”¨æˆ·æŸ¥è¯¢
user_input = st.chat_input("è¯·å¼€å§‹æé—®å§!")

if user_input:
    st.chat_message("user").write(user_input)
    st.session_state.messages.append({"role": "user", "content": user_input})

    with st.chat_message("assistant"):
        st_cb = StreamlitCallbackHandler(st.container())
        with st.spinner("æ­£åœ¨æ€è€ƒä¸­..."):
            try:
                mcp_response = event_loop.run_until_complete(mcp_client.process_query(user_input))
                st.session_state.messages.append({"role": "assistant", "content": mcp_response})
                st.write(mcp_response)
            except Exception as e:
                st.error(f"MCPå·¥å…·è°ƒç”¨å¤±è´¥ï¼š{e}")
